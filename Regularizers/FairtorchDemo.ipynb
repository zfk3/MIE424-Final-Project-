{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 0.0 Notebook Setup\n","\n","We can pip install their library but I think it would be better to use their code as inspiration and develop our own instance. \n","\n","Their code was developed for a hackathon so could have been rushed / contain errors (i.e. has not been rigorously reviewed). "],"metadata":{"id":"gV2rKsShJYEL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ikyi1I9BHozD"},"outputs":[],"source":["#!pip install fairtorch"]},{"cell_type":"code","source":["#from fairtorch import ConstraintLoss, DemographicParityLoss, EqualiedOddsLoss\n","import random\n","import numpy as np\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader, Dataset\n","import os\n","import torch\n","from torch import nn\n","from torch.nn import functional as F"],"metadata":{"id":"Z8-eGBeGIfYx","executionInfo":{"status":"ok","timestamp":1679783088569,"user_tz":240,"elapsed":6,"user":{"displayName":"Rachael Walker","userId":"05271859524918793803"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## 1.0 Code for Regularizers\n","\n","Can be found in the [fairtorch file](https://github.com/wbawakate/fairtorch/tree/master/fairtorch) of the repo in the constraint.py file. \n","\n","\n","Once these regularziers are defined, they can be very simply appended to the gradient descent loss function in pytorch.\n","\n","We define forward() function and backward() will be automatically computed [Source](https://discuss.pytorch.org/t/does-backward-function-call-forward-function/84163).\n","\n","\n","Can also explore: \n","- Equal Odds \n","- Calibration"],"metadata":{"id":"i6uTHxG-JUAi"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","\n","class ConstraintLoss(nn.Module):\n","    def __init__(self, n_class=2, alpha=1, p_norm=2):\n","        super(ConstraintLoss, self).__init__()\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.alpha = alpha\n","        self.p_norm = p_norm\n","        self.n_class = n_class\n","        self.n_constraints = 2\n","        self.dim_condition = self.n_class + 1\n","        self.M = torch.zeros((self.n_constraints, self.dim_condition))\n","        self.c = torch.zeros(self.n_constraints)\n","\n","    def mu_f(self, X=None, y=None, sensitive=None):\n","        return torch.zeros(self.n_constraints)\n","\n","    def forward(self, X, out, sensitive, y=None):\n","        # Reshapes sensitive attribute tensor to the same shape as the output \n","        sensitive = sensitive.view(out.shape)\n","        # Reshapes y (label) tensor to the same shape as the output \n","        if isinstance(y, torch.Tensor):\n","            y = y.view(out.shape)\n","        # Get probability output by applying sigmoid (like logistic regression?)\n","        out = torch.sigmoid(out)\n","        # Get the mu_f value given these tensors \n","        mu = self.mu_f(X=X, out=out, sensitive=sensitive, y=y)\n","        # Gap constraint???\n","        # Apply relu to matrix vector product of M compared to mu-c \n","        # Using cuda if applicable (self.device references)\n","        gap_constraint = F.relu(\n","            torch.mv(self.M.to(self.device), mu.to(self.device)) - self.c.to(self.device)\n","        )\n","        # Using the L2 Norm of the gap constraint as regularizer with alpha parameter. \n","        if self.p_norm == 2:\n","            cons = self.alpha * torch.dot(gap_constraint, gap_constraint)\n","        else:\n","            cons = self.alpha * torch.dot(gap_constraint.detach(), gap_constraint)\n","        return cons\n","\n","\n","class DemographicParityLoss(ConstraintLoss):\n","    def __init__(self, sensitive_classes=[0, 1], alpha=1, p_norm=2):\n","        \"\"\"loss of demograpfhic parity\n","\n","        Args:\n","            sensitive_classes (list, optional): list of unique values of sensitive attribute. Defaults to [0, 1].\n","            alpha (int, optional): [description]. Defaults to 1.\n","            p_norm (int, optional): [description]. Defaults to 2.\n","        \"\"\"\n","        self.sensitive_classes = sensitive_classes\n","        self.n_class = len(sensitive_classes)\n","        super(DemographicParityLoss, self).__init__(\n","            n_class=self.n_class, alpha=alpha, p_norm=p_norm\n","        )\n","        \n","        self.n_constraints = 2 * self.n_class\n","        self.dim_condition = self.n_class + 1\n","        self.M = torch.zeros((self.n_constraints, self.dim_condition))\n","        for i in range(self.n_constraints):\n","            j = i % 2\n","            if j == 0:\n","                self.M[i, j] = 1.0\n","                self.M[i, -1] = -1.0\n","            else:\n","                self.M[i, j - 1] = -1.0\n","                self.M[i, -1] = 1.0\n","            print(self.M)\n","        self.c = torch.zeros(self.n_constraints)\n","\n","    def mu_f(self, X, out, sensitive, y=None):\n","        expected_values_list = []\n","        for v in self.sensitive_classes:\n","            # Get the index for each of the senstive classes \n","            idx_true = sensitive == v  # torch.bool\n","            # Get the average prediction for that sensitive class\n","            expected_values_list.append(out[idx_true].mean())\n","        # Append the overall mean to the expected values list\n","        print(expected_values_list) \n","        expected_values_list.append(out.mean())\n","        return torch.stack(expected_values_list)\n","\n","    def forward(self, X, out, sensitive, y=None):\n","        return super(DemographicParityLoss, self).forward(X, out, sensitive)\n","\n","# Convex \n","class EqualiedOddsLoss(ConstraintLoss):\n","    def __init__(self, sensitive_classes=[0, 1], alpha=1, p_norm=2):\n","        \"\"\"loss of demograpfhic parity\n","\n","        Args:\n","            sensitive_classes (list, optional): list of unique values of sensitive attribute. Defaults to [0, 1].\n","            alpha (int, optional): [description]. Defaults to 1.\n","            p_norm (int, optional): [description]. Defaults to 2.\n","        \"\"\"\n","        self.sensitive_classes = sensitive_classes\n","        self.y_classes = [0, 1]\n","        self.n_class = len(sensitive_classes)\n","        self.n_y_class = len(self.y_classes)\n","        super(EqualiedOddsLoss, self).__init__(n_class=self.n_class, alpha=alpha, p_norm=p_norm)\n","        # K:  number of constraint : (|A| x |Y| x {+, -})\n","        self.n_constraints = self.n_class * self.n_y_class * 2\n","        # J : dim of conditions  : ((|A|+1) x |Y|)\n","        self.dim_condition = self.n_y_class * (self.n_class + 1)\n","        self.M = torch.zeros((self.n_constraints, self.dim_condition))\n","        # make M (K * J): (|A| x |Y| x {+, -})  *   (|A|+1) x |Y|) )\n","        self.c = torch.zeros(self.n_constraints)\n","        element_K_A = self.sensitive_classes + [None]\n","        for i_a, a_0 in enumerate(self.sensitive_classes):\n","            for i_y, y_0 in enumerate(self.y_classes):\n","                for i_s, s in enumerate([-1, 1]):\n","                    for j_y, y_1 in enumerate(self.y_classes):\n","                        for j_a, a_1 in enumerate(element_K_A):\n","                            i = i_a * (2 * self.n_y_class) + i_y * 2 + i_s\n","                            j = j_y + self.n_y_class * j_a\n","                            self.M[i, j] = self.__element_M(a_0, a_1, y_1, y_1, s)\n","\n","    def __element_M(self, a0, a1, y0, y1, s):\n","        if a0 is None or a1 is None:\n","            x = y0 == y1\n","            return -1 * s * x\n","        else:\n","            x = (a0 == a1) & (y0 == y1)\n","            return s * float(x)\n","\n","    def mu_f(self, X, out, sensitive, y):\n","        expected_values_list = []\n","        for u in self.sensitive_classes:\n","            for v in self.y_classes:\n","                idx_true = (y == v) * (sensitive == u)  # torch.bool\n","                expected_values_list.append(out[idx_true].mean())\n","        # sensitive is star\n","        for v in self.y_classes:\n","            idx_true = y == v\n","            expected_values_list.append(out[idx_true].mean())\n","        return torch.stack(expected_values_list)\n","\n","    def forward(self, X, out, sensitive, y):\n","        return super(EqualiedOddsLoss, self).forward(X, out, sensitive, y=y)"],"metadata":{"id":"xyg2kvi5Ij69","executionInfo":{"status":"ok","timestamp":1679783088568,"user_tz":240,"elapsed":8508,"user":{"displayName":"Rachael Walker","userId":"05271859524918793803"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# What is M? \n","n_class=2\n","n_constraints = 2 * n_class\n","dim_condition = n_class + 1\n","M = torch.zeros((n_constraints, dim_condition))\n","for i in range(n_constraints):\n","    j = i % 2\n","    if j == 0:\n","        M[i, j] = 1.0\n","        M[i, -1] = -1.0\n","    else:\n","        M[i, j - 1] = -1.0\n","        M[i, -1] = 1.0\n","print(M)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aL4HYF0D5lpX","executionInfo":{"status":"ok","timestamp":1679783154523,"user_tz":240,"elapsed":301,"user":{"displayName":"Rachael Walker","userId":"05271859524918793803"}},"outputId":"82c6bd61-9e9f-40fd-890f-128015ea3e21"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.,  0., -1.],\n","        [-1.,  0.,  1.],\n","        [ 1.,  0., -1.],\n","        [-1.,  0.,  1.]])\n"]}]},{"cell_type":"markdown","source":["**Equalized Odds:** This loss function ensures that the model produces similar false positive and false negative rates across different groups. Mathematically, it can be expressed as the sum of absolute differences between false positive rates and false negative rates across different groups.\n","\n","**Demographic Parity:** This loss function ensures that the model produces similar probabilities of positive outcome across different groups. Mathematically, it can be expressed as the absolute difference between the probabilities of positive outcome for different groups.\n","\n","**Calibration:** This loss function ensures that the model produces similar predicted probabilities of positive outcome and actual probabilities of positive outcome across different groups. Mathematically, it can be expressed as the sum of squared differences between predicted probabilities and actual probabilities of positive outcome across different groups.\n","\n","**Bounded Group Loss:** This loss function ensures that the model produces similar performance for different groups while still achieving high overall performance. Mathematically, it can be expressed as a combination of accuracy and a group fairness metric, such as the difference between false positive rates or the difference between positive predictive values for different groups.\n"],"metadata":{"id":"u-yF9v9dY6-v"}},{"cell_type":"markdown","source":["## 2.0 Prep Artificial Dataset"],"metadata":{"id":"5hYWlJ0HJ7iB"}},{"cell_type":"code","source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","seed_everything(2020)"],"metadata":{"id":"uhmO_NhoJKL5","executionInfo":{"status":"ok","timestamp":1679783156251,"user_tz":240,"elapsed":183,"user":{"displayName":"Rachael Walker","userId":"05271859524918793803"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["n_samples = 512\n","n_feature = 4\n","\n","def genelate_data(n_samples = n_samples, n_feature=n_feature):\n","\n","    y = np.random.randint(0, 2, size=n_samples)\n","    loc0 = np.random.uniform(-1, 1, n_feature)\n","    loc1 = np.random.uniform(-1, 1, n_feature)\n","\n","    X = np.zeros((n_samples, n_feature))\n","    for i, u in enumerate(y):\n","        if y[i] ==0:\n","            X[i] = np.random.normal(loc = loc0, scale=1.0, size=n_feature)  \n","        else:\n","            X[i] = np.random.normal(loc = loc1, scale=1.0, size=n_feature)  \n","\n","    sensi_feat = (X[:, 0] > X[:, 0].mean()).astype(int)\n","    X[:, 0] = sensi_feat.astype(np.float32)\n","    X = torch.from_numpy(X).float()\n","    y = torch.from_numpy(y).float()\n","    sensi_feat = torch.from_numpy(sensi_feat)\n","    return X, y, sensi_feat"],"metadata":{"id":"cW0SP5swKEoQ","executionInfo":{"status":"ok","timestamp":1679783157332,"user_tz":240,"elapsed":110,"user":{"displayName":"Rachael Walker","userId":"05271859524918793803"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["dataset = genelate_data(1024)\n","# data split\n","n_train = int(0.7*len(dataset[0]))\n","X_train, y_train, sensi_train = map(lambda x : x[:n_train], dataset)\n","X_test, y_test, sensi_test = map(lambda x : x[n_train:], dataset)"],"metadata":{"id":"LVhg81vOKPU5","executionInfo":{"status":"ok","timestamp":1679783159380,"user_tz":240,"elapsed":112,"user":{"displayName":"Rachael Walker","userId":"05271859524918793803"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["model = nn.Sequential(nn.Linear(n_feature,1))\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.SGD(model.parameters(),lr=0.1)\n","\n","for i in range(0, 200):\n","    optimizer.zero_grad()    \n","    logit = model(X_train)\n","    loss = criterion(logit.view(-1), y_train)\n","    \n","    loss.backward()\n","    optimizer.step()\n","y_pred = (torch.sigmoid(model(X_test)).view(-1) > 0.5 ).float()\n","acc_test = (y_pred  == y_test ).float().mean().item()\n","\n","print(\"acc test: \",acc_test)\n","\n","acc_test_vanilla = acc_test\n","\n","gap_vanilla = np.abs(y_pred[sensi_test==0].mean().item() - y_pred[sensi_test==1].mean().item())\n","print(\"gap of expected values: \", gap_vanilla)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BDDUhoE0KUfZ","executionInfo":{"status":"ok","timestamp":1679515323464,"user_tz":240,"elapsed":348,"user":{"displayName":"Scott Oxholm","userId":"04662700780988494317"}},"outputId":"c0482ed1-55db-44f0-c7e3-a1a1e31220ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["acc test:  0.8376623392105103\n","gap of expected values:  0.686927929520607\n"]}]},{"cell_type":"code","source":["criterion = nn.BCEWithLogitsLoss()\n","model = nn.Sequential(nn.Linear(n_feature,1))\n","optimizer = optim.SGD(model.parameters(),lr=0.1)\n","criterion.__class__.__name__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"bhj6v5zj4dqC","executionInfo":{"status":"ok","timestamp":1679783527311,"user_tz":240,"elapsed":122,"user":{"displayName":"Rachael Walker","userId":"05271859524918793803"}},"outputId":"20be42cf-cd36-404c-ce91-aab5cdaa1afc"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'BCEWithLogitsLoss'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["model = nn.Sequential(nn.Linear(n_feature,1))\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.SGD(model.parameters(),lr=0.1)\n","\n","regularizers = []\n","\n","\n","\n","for i in range(0, 200):\n","    optimizer.zero_grad()    \n","    logit = model(X_train)\n","    loss = criterion(logit.view(-1), y_train)\n","    "],"metadata":{"id":"ZZFOB6N14WRJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dim_hiden = 32\n","model = nn.Sequential(nn.Linear(n_feature,1))\n","\n","dp_loss = DemographicParityLoss(sensitive_classes=[0, 1], alpha=100) # constraint \n","optimizer = optim.SGD(model.parameters(),lr=0.1)\n","\n","\n"," \n","# train \n","for i in range(0, 100):\n","    optimizer.zero_grad()    \n","    logit = model(X_train)\n","    loss = criterion(logit.view(-1), y_train)\n","    loss +=  dp_loss(X_train, logit, sensi_train) # add constraint\n","    loss.backward()\n","    optimizer.step()\n","y_pred = (torch.sigmoid(model(X_test)).view(-1) > 0.5 ).float()\n","acc_test = (y_pred  == y_test ).float().mean().float().item()\n","\n","print(\"acc test: \",acc_test)\n","\n","acc_test_vanilla = acc_test\n","\n","gap_dp = np.abs(y_pred[sensi_test==0].mean().item() - y_pred[sensi_test==1].mean().item())\n","print(\"gap of expected values: \", gap_dp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1a4V8x0K9B3","executionInfo":{"status":"ok","timestamp":1679515326210,"user_tz":240,"elapsed":1293,"user":{"displayName":"Scott Oxholm","userId":"04662700780988494317"}},"outputId":"b59ca103-7e0f-4447-cfe8-bdc3e056f592"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.,  0., -1.],\n","        [ 0.,  0.,  0.],\n","        [ 0.,  0.,  0.],\n","        [ 0.,  0.,  0.]])\n","tensor([[ 1.,  0., -1.],\n","        [-1.,  0.,  1.],\n","        [ 0.,  0.,  0.],\n","        [ 0.,  0.,  0.]])\n","tensor([[ 1.,  0., -1.],\n","        [-1.,  0.,  1.],\n","        [ 1.,  0., -1.],\n","        [ 0.,  0.,  0.]])\n","tensor([[ 1.,  0., -1.],\n","        [-1.,  0.,  1.],\n","        [ 1.,  0., -1.],\n","        [-1.,  0.,  1.]])\n","[tensor(0.5046, grad_fn=<MeanBackward0>), tensor(0.4542, grad_fn=<MeanBackward0>)]\n","[tensor(0.4933, grad_fn=<MeanBackward0>), tensor(0.4737, grad_fn=<MeanBackward0>)]\n","[tensor(0.4957, grad_fn=<MeanBackward0>), tensor(0.4828, grad_fn=<MeanBackward0>)]\n","[tensor(0.5006, grad_fn=<MeanBackward0>), tensor(0.4893, grad_fn=<MeanBackward0>)]\n","[tensor(0.5057, grad_fn=<MeanBackward0>), tensor(0.4948, grad_fn=<MeanBackward0>)]\n","[tensor(0.5105, grad_fn=<MeanBackward0>), tensor(0.4997, grad_fn=<MeanBackward0>)]\n","[tensor(0.5148, grad_fn=<MeanBackward0>), tensor(0.5043, grad_fn=<MeanBackward0>)]\n","[tensor(0.5189, grad_fn=<MeanBackward0>), tensor(0.5084, grad_fn=<MeanBackward0>)]\n","[tensor(0.5225, grad_fn=<MeanBackward0>), tensor(0.5122, grad_fn=<MeanBackward0>)]\n","[tensor(0.5259, grad_fn=<MeanBackward0>), tensor(0.5156, grad_fn=<MeanBackward0>)]\n","[tensor(0.5289, grad_fn=<MeanBackward0>), tensor(0.5188, grad_fn=<MeanBackward0>)]\n","[tensor(0.5317, grad_fn=<MeanBackward0>), tensor(0.5216, grad_fn=<MeanBackward0>)]\n","[tensor(0.5342, grad_fn=<MeanBackward0>), tensor(0.5242, grad_fn=<MeanBackward0>)]\n","[tensor(0.5365, grad_fn=<MeanBackward0>), tensor(0.5266, grad_fn=<MeanBackward0>)]\n","[tensor(0.5385, grad_fn=<MeanBackward0>), tensor(0.5287, grad_fn=<MeanBackward0>)]\n","[tensor(0.5404, grad_fn=<MeanBackward0>), tensor(0.5306, grad_fn=<MeanBackward0>)]\n","[tensor(0.5421, grad_fn=<MeanBackward0>), tensor(0.5323, grad_fn=<MeanBackward0>)]\n","[tensor(0.5436, grad_fn=<MeanBackward0>), tensor(0.5339, grad_fn=<MeanBackward0>)]\n","[tensor(0.5449, grad_fn=<MeanBackward0>), tensor(0.5353, grad_fn=<MeanBackward0>)]\n","[tensor(0.5461, grad_fn=<MeanBackward0>), tensor(0.5366, grad_fn=<MeanBackward0>)]\n","[tensor(0.5472, grad_fn=<MeanBackward0>), tensor(0.5377, grad_fn=<MeanBackward0>)]\n","[tensor(0.5481, grad_fn=<MeanBackward0>), tensor(0.5387, grad_fn=<MeanBackward0>)]\n","[tensor(0.5490, grad_fn=<MeanBackward0>), tensor(0.5396, grad_fn=<MeanBackward0>)]\n","[tensor(0.5497, grad_fn=<MeanBackward0>), tensor(0.5403, grad_fn=<MeanBackward0>)]\n","[tensor(0.5503, grad_fn=<MeanBackward0>), tensor(0.5410, grad_fn=<MeanBackward0>)]\n","[tensor(0.5509, grad_fn=<MeanBackward0>), tensor(0.5416, grad_fn=<MeanBackward0>)]\n","[tensor(0.5514, grad_fn=<MeanBackward0>), tensor(0.5421, grad_fn=<MeanBackward0>)]\n","[tensor(0.5518, grad_fn=<MeanBackward0>), tensor(0.5426, grad_fn=<MeanBackward0>)]\n","[tensor(0.5521, grad_fn=<MeanBackward0>), tensor(0.5430, grad_fn=<MeanBackward0>)]\n","[tensor(0.5524, grad_fn=<MeanBackward0>), tensor(0.5433, grad_fn=<MeanBackward0>)]\n","[tensor(0.5526, grad_fn=<MeanBackward0>), tensor(0.5436, grad_fn=<MeanBackward0>)]\n","[tensor(0.5528, grad_fn=<MeanBackward0>), tensor(0.5438, grad_fn=<MeanBackward0>)]\n","[tensor(0.5530, grad_fn=<MeanBackward0>), tensor(0.5439, grad_fn=<MeanBackward0>)]\n","[tensor(0.5531, grad_fn=<MeanBackward0>), tensor(0.5441, grad_fn=<MeanBackward0>)]\n","[tensor(0.5531, grad_fn=<MeanBackward0>), tensor(0.5442, grad_fn=<MeanBackward0>)]\n","[tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5442, grad_fn=<MeanBackward0>)]\n","[tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5442, grad_fn=<MeanBackward0>)]\n","[tensor(0.5531, grad_fn=<MeanBackward0>), tensor(0.5442, grad_fn=<MeanBackward0>)]\n","[tensor(0.5531, grad_fn=<MeanBackward0>), tensor(0.5442, grad_fn=<MeanBackward0>)]\n","[tensor(0.5530, grad_fn=<MeanBackward0>), tensor(0.5442, grad_fn=<MeanBackward0>)]\n","[tensor(0.5529, grad_fn=<MeanBackward0>), tensor(0.5441, grad_fn=<MeanBackward0>)]\n","[tensor(0.5528, grad_fn=<MeanBackward0>), tensor(0.5440, grad_fn=<MeanBackward0>)]\n","[tensor(0.5526, grad_fn=<MeanBackward0>), tensor(0.5439, grad_fn=<MeanBackward0>)]\n","[tensor(0.5525, grad_fn=<MeanBackward0>), tensor(0.5437, grad_fn=<MeanBackward0>)]\n","[tensor(0.5523, grad_fn=<MeanBackward0>), tensor(0.5436, grad_fn=<MeanBackward0>)]\n","[tensor(0.5521, grad_fn=<MeanBackward0>), tensor(0.5434, grad_fn=<MeanBackward0>)]\n","[tensor(0.5519, grad_fn=<MeanBackward0>), tensor(0.5433, grad_fn=<MeanBackward0>)]\n","[tensor(0.5517, grad_fn=<MeanBackward0>), tensor(0.5431, grad_fn=<MeanBackward0>)]\n","[tensor(0.5515, grad_fn=<MeanBackward0>), tensor(0.5429, grad_fn=<MeanBackward0>)]\n","[tensor(0.5513, grad_fn=<MeanBackward0>), tensor(0.5427, grad_fn=<MeanBackward0>)]\n","[tensor(0.5511, grad_fn=<MeanBackward0>), tensor(0.5425, grad_fn=<MeanBackward0>)]\n","[tensor(0.5509, grad_fn=<MeanBackward0>), tensor(0.5423, grad_fn=<MeanBackward0>)]\n","[tensor(0.5506, grad_fn=<MeanBackward0>), tensor(0.5421, grad_fn=<MeanBackward0>)]\n","[tensor(0.5504, grad_fn=<MeanBackward0>), tensor(0.5419, grad_fn=<MeanBackward0>)]\n","[tensor(0.5502, grad_fn=<MeanBackward0>), tensor(0.5416, grad_fn=<MeanBackward0>)]\n","[tensor(0.5499, grad_fn=<MeanBackward0>), tensor(0.5414, grad_fn=<MeanBackward0>)]\n","[tensor(0.5497, grad_fn=<MeanBackward0>), tensor(0.5412, grad_fn=<MeanBackward0>)]\n","[tensor(0.5494, grad_fn=<MeanBackward0>), tensor(0.5409, grad_fn=<MeanBackward0>)]\n","[tensor(0.5492, grad_fn=<MeanBackward0>), tensor(0.5407, grad_fn=<MeanBackward0>)]\n","[tensor(0.5489, grad_fn=<MeanBackward0>), tensor(0.5404, grad_fn=<MeanBackward0>)]\n","[tensor(0.5486, grad_fn=<MeanBackward0>), tensor(0.5402, grad_fn=<MeanBackward0>)]\n","[tensor(0.5484, grad_fn=<MeanBackward0>), tensor(0.5400, grad_fn=<MeanBackward0>)]\n","[tensor(0.5481, grad_fn=<MeanBackward0>), tensor(0.5397, grad_fn=<MeanBackward0>)]\n","[tensor(0.5479, grad_fn=<MeanBackward0>), tensor(0.5395, grad_fn=<MeanBackward0>)]\n","[tensor(0.5476, grad_fn=<MeanBackward0>), tensor(0.5392, grad_fn=<MeanBackward0>)]\n","[tensor(0.5473, grad_fn=<MeanBackward0>), tensor(0.5390, grad_fn=<MeanBackward0>)]\n","[tensor(0.5471, grad_fn=<MeanBackward0>), tensor(0.5387, grad_fn=<MeanBackward0>)]\n","[tensor(0.5468, grad_fn=<MeanBackward0>), tensor(0.5385, grad_fn=<MeanBackward0>)]\n","[tensor(0.5466, grad_fn=<MeanBackward0>), tensor(0.5382, grad_fn=<MeanBackward0>)]\n","[tensor(0.5463, grad_fn=<MeanBackward0>), tensor(0.5380, grad_fn=<MeanBackward0>)]\n","[tensor(0.5460, grad_fn=<MeanBackward0>), tensor(0.5377, grad_fn=<MeanBackward0>)]\n","[tensor(0.5458, grad_fn=<MeanBackward0>), tensor(0.5375, grad_fn=<MeanBackward0>)]\n","[tensor(0.5455, grad_fn=<MeanBackward0>), tensor(0.5372, grad_fn=<MeanBackward0>)]\n","[tensor(0.5453, grad_fn=<MeanBackward0>), tensor(0.5370, grad_fn=<MeanBackward0>)]\n","[tensor(0.5450, grad_fn=<MeanBackward0>), tensor(0.5368, grad_fn=<MeanBackward0>)]\n","[tensor(0.5448, grad_fn=<MeanBackward0>), tensor(0.5365, grad_fn=<MeanBackward0>)]\n","[tensor(0.5445, grad_fn=<MeanBackward0>), tensor(0.5363, grad_fn=<MeanBackward0>)]\n","[tensor(0.5443, grad_fn=<MeanBackward0>), tensor(0.5360, grad_fn=<MeanBackward0>)]\n","[tensor(0.5440, grad_fn=<MeanBackward0>), tensor(0.5358, grad_fn=<MeanBackward0>)]\n","[tensor(0.5438, grad_fn=<MeanBackward0>), tensor(0.5355, grad_fn=<MeanBackward0>)]\n","[tensor(0.5435, grad_fn=<MeanBackward0>), tensor(0.5353, grad_fn=<MeanBackward0>)]\n","[tensor(0.5433, grad_fn=<MeanBackward0>), tensor(0.5351, grad_fn=<MeanBackward0>)]\n","[tensor(0.5430, grad_fn=<MeanBackward0>), tensor(0.5348, grad_fn=<MeanBackward0>)]\n","[tensor(0.5428, grad_fn=<MeanBackward0>), tensor(0.5346, grad_fn=<MeanBackward0>)]\n","[tensor(0.5425, grad_fn=<MeanBackward0>), tensor(0.5344, grad_fn=<MeanBackward0>)]\n","[tensor(0.5423, grad_fn=<MeanBackward0>), tensor(0.5341, grad_fn=<MeanBackward0>)]\n","[tensor(0.5421, grad_fn=<MeanBackward0>), tensor(0.5339, grad_fn=<MeanBackward0>)]\n","[tensor(0.5418, grad_fn=<MeanBackward0>), tensor(0.5337, grad_fn=<MeanBackward0>)]\n","[tensor(0.5416, grad_fn=<MeanBackward0>), tensor(0.5335, grad_fn=<MeanBackward0>)]\n","[tensor(0.5414, grad_fn=<MeanBackward0>), tensor(0.5332, grad_fn=<MeanBackward0>)]\n","[tensor(0.5411, grad_fn=<MeanBackward0>), tensor(0.5330, grad_fn=<MeanBackward0>)]\n","[tensor(0.5409, grad_fn=<MeanBackward0>), tensor(0.5328, grad_fn=<MeanBackward0>)]\n","[tensor(0.5407, grad_fn=<MeanBackward0>), tensor(0.5326, grad_fn=<MeanBackward0>)]\n","[tensor(0.5404, grad_fn=<MeanBackward0>), tensor(0.5324, grad_fn=<MeanBackward0>)]\n","[tensor(0.5402, grad_fn=<MeanBackward0>), tensor(0.5321, grad_fn=<MeanBackward0>)]\n","[tensor(0.5400, grad_fn=<MeanBackward0>), tensor(0.5319, grad_fn=<MeanBackward0>)]\n","[tensor(0.5398, grad_fn=<MeanBackward0>), tensor(0.5317, grad_fn=<MeanBackward0>)]\n","[tensor(0.5395, grad_fn=<MeanBackward0>), tensor(0.5315, grad_fn=<MeanBackward0>)]\n","[tensor(0.5393, grad_fn=<MeanBackward0>), tensor(0.5313, grad_fn=<MeanBackward0>)]\n","[tensor(0.5391, grad_fn=<MeanBackward0>), tensor(0.5311, grad_fn=<MeanBackward0>)]\n","acc test:  0.5941558480262756\n","gap of expected values:  0.0016872286796569824\n"]}]}]}